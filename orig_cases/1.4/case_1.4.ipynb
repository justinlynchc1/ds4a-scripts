{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How are returns and volatility related for energy stocks and the broader market?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Context.** You are an analyst at a large bank focused on natural resource stock investments. You recently conducted an analysis of the following energy stocks and how their trading volume is related to their volatility:\n",
    "\n",
    "1. Dominion Energy Inc. (Stock Symbol: D)\n",
    "2. Exelon Corp. (Stock Symbol: EXC)\n",
    "3. NextEra Energy Inc. (Stock Symbol: NEE)\n",
    "4. Southern Co. (Stock Symbol: SO)\n",
    "5. Duke Energy Corp. (Stock Symbol: DUK)\n",
    "\n",
    "Your boss was quite pleased with your previous analysis, and now wants you to conduct additional analysis so he can figure out how to size potential positions in these stocks; i.e. what percentage of the investment portfolio should be dedicated to each of these stocks. Specifically, he wants you to look at daily returns and volatility for each stock as well as for the broader (i.e. not just the energy sector) market.\n",
    "\n",
    "This is important because high volatility implies higher risk, and your boss would like to know if the potential returns of these high-volatility energy stocks compensate him for the added risk. Additionally, because his performance is measured against (i.e. benchmarked) the broader market, he wants to understand whether these stocks generally outperform the broader market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Problem.** Based on the above context, your boss has posed the following question to you: **\"What is the relationship between daily volatility and returns for these stocks, and what is the relationship between daily returns for these stocks and the broader stock market?\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analytical Context.** The data you've been given is in the Comma Separated Value (CSV) format, and comprises price and trading volume data for the above stocks. You will proceed by: (1) conducting preliminary cleaning of the data; (2) creating additional features required for our analysis; (3) labelling the data into volatility groups (i.e. regimes) and determining how volatility is related to returns; and finally (4) comparing these returns against those of the broader market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries required for this case\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary cleaning of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before one can proceed with data analysis and modeling, one first needs to determine if the relevant data is adequate to proceed as-is, or if it needs further cleaning. In this case, we have received a Comma Separated Value (CSV) file that includes the following data:\n",
    "\n",
    "1. **Date:** The day of the year\n",
    "2. **Open:** The stock opening price of the day\n",
    "3. **High:** The highest observed stock price of the day\n",
    "4. **Low:** The lowest observed stock price of the day\n",
    "5. **Close:** The stock closing price of the day\n",
    "6. **Adj Close:** The adjusted stock closing price for the day (adjusted for splits and dividends)\n",
    "7. **Volume:** The volume of the stock traded over the day\n",
    "8. **Symbol:** The symbol for that particular stock\n",
    "\n",
    "One very common problem that arises in datasets is missing values. Let's see how to identify whether or not our dataset has this problem, and how to deal with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'EnergySectorData.csv' does not exist: b'EnergySectorData.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ed54a6ef25bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load and view head of DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mraw_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'EnergySectorData.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mraw_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'EnergySectorData.csv' does not exist: b'EnergySectorData.csv'"
     ]
    }
   ],
   "source": [
    "# Load and view head of DataFrame\n",
    "raw_df = pd.read_csv('EnergySectorData.csv')\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by determining if we have missing values. We can use the ```pandas``` DataFrame method ```isnull()``` to check for ```NaN``` values in ```raw_df``` (i.e. check for null values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if there are missing values (NaNs)\n",
    "raw_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see we have some missing values, let's instead use the ```mean()``` method to determine what percent of each column we are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.isnull().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that we are missing less than 0.5% of the observations in any given column. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1:\n",
    "We do not want any missing values in our analysis. Which of the following options is the WORST option on how to proceed in this case?\n",
    "\n",
    "(a) Fill in any day's missing value with the previous days value\n",
    "\n",
    "(b) Replace the missing values by re-gathering the data\n",
    "\n",
    "(c) Estimate the missing values by interpolating them from the values of other, similar data points\n",
    "\n",
    "(d) Remove rows from the dataset that contain missing values\n",
    "\n",
    "**Answer.** (a). We have a couple options that are generally acceptable on how to proceed with missing values:\n",
    "\n",
    "1. One option is to replace the missing values by re-gathering the data. However, this option is often quite expensive in real man-hours, so we will forgo it for now.\n",
    "\n",
    "2. Another option is to try to estimate the missing values using some reasonable estimation method by interpolating from other data point. However, this can be complicated and given that such a little amount of our data is missing, we will forgo this option.\n",
    "\n",
    "3. In practice, a regularly chosen option when only a small amount of data is missing is to just remove the rows that have missing data. This option is generally fine to perform so long as the removed data is an insignificant portion of the data under study. Here we will choose this option as it simplifies the analysis and should not harm any results moving forward.\n",
    "\n",
    "Answer (a) is problematic because replacing a missing value with the previous day's value doesn't make sense for stocks because stock prices and trading volumes are known to move day-to-day rather than stay unchanged for an extended period of time. Since we will be dealing with daily returns and volatility, this is especially problematic as it defaults any missing day's volatility and return to 0.\n",
    "\n",
    "Note that these options for cleaning data should be carefully weighed when commencing a new data science study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean the missing values by removing them from the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaNs from data\n",
    "# Drop the missing values\n",
    "progress_df = raw_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to be able to analyze these stocks together across time. One thing that would make this easier is if all the stocks contained non-missing data for the same set of dates. Let's first ascertain if this is the case. One way to do this is to use the ```groupby``` method to group by ```Date```, then use the ```count()``` function to enumerate how many distinct dates we have. Since there are a total of 1259 rows per symbol, there should be a count of 1259 for each symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How many data rows do we have for each Symbol\n",
    "progress_df.groupby('Symbol').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since most symbols do not have a count of 1259 for their ```Date``` columns, there are clearly some inconsistent values. Some of these duplicates will be missing values (NaNs), so let's enumerate those again first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if there are missing values (NaNs)\n",
    "raw_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed earlier, we can remove missing values as there are not many samples that are missing, and dropping a small number of dates is not expected to significantly impact the analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the missing values\n",
    "progress_df = raw_df.dropna().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How many data rows do we have for each Symbol\n",
    "progress_df.groupby('Symbol').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, we see that different symbols have different numbers of dates. We'd like all the symbols to have the same set of dates for analysis purposes. Let's create a new ```clean_df``` that corresponds to a DataFrame with the same number of rows for each ```Symbol```, where all symbols share the same set of dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_dates_D = set(progress_df[progress_df['Symbol'] == 'D']['Date'])\n",
    "set_dates_EXC = set(progress_df[progress_df['Symbol'] == 'EXC']['Date'])\n",
    "set_dates_NEE = set(progress_df[progress_df['Symbol'] == 'NEE']['Date'])\n",
    "set_dates_SO = set(progress_df[progress_df['Symbol'] == 'SO']['Date'])\n",
    "set_dates_DUK = set(progress_df[progress_df['Symbol'] == 'DUK']['Date'])\n",
    "set_unique_dates = set.intersection(set_dates_D,set_dates_EXC,set_dates_NEE,set_dates_SO,set_dates_DUK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter new DataFrame for only the dates that are present in every symbol (i.e. the overlapping dates)\n",
    "clean_df = progress_df[progress_df['Date'].isin(set_unique_dates)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look\n",
    "clean_df.groupby('Symbol').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that each symbol has the same number of unique dates. Let's write a quick verification program to ensure the resulting ```clean_df``` does indeed have the same dates for every symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2:\n",
    "Write code to ensure that each of the symbols share the same set of unique dates. (Hint: use the ```set()``` method.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One possible solution\n",
    "check_set_dates_D = set(clean_df[clean_df['Symbol'] == 'D']['Date'])\n",
    "check_set_dates_EXC = set(clean_df[clean_df['Symbol'] == 'EXC']['Date'])\n",
    "check_set_dates_NEE = set(clean_df[clean_df['Symbol'] == 'NEE']['Date'])\n",
    "check_set_dates_SO = set(clean_df[clean_df['Symbol'] == 'SO']['Date'])\n",
    "check_set_dates_DUK = set(clean_df[clean_df['Symbol'] == 'DUK']['Date'])\n",
    "\n",
    "print(check_set_dates_D == check_set_dates_EXC)\n",
    "print(check_set_dates_D == check_set_dates_NEE)\n",
    "print(check_set_dates_D == check_set_dates_SO)\n",
    "print(check_set_dates_D == check_set_dates_DUK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've completed the preliminary cleaning of the data, let's move forward with determining the relationships between: (1) stock returns and volatility, and (2) stock returns and broader market returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding additional variables required for our analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the original question requires us to investigate both daily stocks returns as well as the volatility of those returns. This means that important measures of interest are:\n",
    "\n",
    "1. Daily (open to close) stock return\n",
    "2. Volatility of daily stock return\n",
    "\n",
    "Why are each of these important?\n",
    "\n",
    "1. Volatility: Gives insight into amount of price movement in any given day. Volatility is directly related to the level of risk involved in holding the stock.\n",
    "2. Return: Gives us an idea of the return on investment over a period of time.\n",
    "\n",
    "Let's calculate these statistics and add them to the DataFrame ```clean_df```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['VolStat'] = (clean_df['High'] - clean_df['Low']) / clean_df['Open']\n",
    "clean_df['Return'] = (clean_df['Close'] / clean_df['Open']) - 1.0\n",
    "clean_df['Volume_Millions'] = clean_df['Volume'] / 1000000.0 # Volume in Millions (added for convenience)\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that we've added three columns to ```clean_df```, namely ```VolStat```, ```Return```, and ```Volume_Millions``` (the last one is just for convenience, as the values in the ```Volume``` column are quite large)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are looking to analyze the relationship between daily volatility and returns, one additional column that makes sense to add is one which says ```True``` when the daily return is positive, and ```False``` when the daily return negative. We can then group days into positive and negative return cohorts and compare the average volatility on those days. We will name this column ```ReturnFlag```.\n",
    "\n",
    "We can accomplish this by using an **anonymous function**; that is, a function that is defined but not named:\n",
    "```python\n",
    "lambda arguments: expression\n",
    "```\n",
    "\n",
    "The ```lambda``` keyword tells Python that we are using an anonymous function. Next, the ```arguments``` are the name we give to the inputs. It can be ```x```, or ```y```, or whatever the user would like to call it. In this case we will use the name ```row``` for the input argument name as the input will indeed be a row of a DataFrame. The ```expression``` is what is then applied to the ```arguments```; this is the function.\n",
    "\n",
    "Let's take a look at how we can use anonymous functions to create the ```ReturnFlag``` feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df['ReturnFlag'] = clean_df.apply(lambda row: True if row['Return'] > 0 else False, axis=1) # Volume in Millions\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the ```apply()``` method takes in an anonymous function, and applies it to the rows of the DataFrame through the use of the second argument ```axis```. ```axis=0``` applies the function to columns, whereas ```axis=1``` applies the function to rows.\n",
    "\n",
    "So what is happening in the following statement?\n",
    "```python\n",
    "clean_df['ReturnFlag'] = clean_df.apply(lambda row: True if row['Return] > 0 else False, axis=1)\n",
    "```\n",
    "\n",
    "1. ```pandas``` recogized through the ```apply``` method that it is operating on the ```clean_df``` DataFrame\n",
    "2. The ```apply``` method takes a function as input that will applied to the DataFrame ```clean_df```\n",
    "3. Given the second argument of ```apply``` is ```axis=1``` the input into the anonymous function is a single row.\n",
    "4. For every row, ```row['Return']``` returns the ```Return``` value for that row, and it is subsequently passed through the if statement, returning True if greater than zero and False otherwise.\n",
    "5. The new value is stored in the column ```clean_df['ReturnFlag']```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "\n",
    "Using ```apply()``` and ```lambda```, write code to create a new column named ```YYYY``` to ```clean_df```, where the new column is the year of the observation as a string. For instance if the row ```Date``` value is 2014-07-28, then the value in the new column for the year would be '2014'. Recall you can access the first 4 characters of some string ```my_string``` using ```my_string[:4]```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One possible solution\n",
    "clean_df['YYYY'] = clean_df.apply(lambda row: row['Date'][:4], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move forward with labelling volatility regimes present in the data -  these regimes are useful for breaking down the stock return analysis by periods of low, medium, and high volatility. It will allow for more granular analysis than just looking at overall averages without a breakdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labelling energy sector volatility regimes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Case 1.2, we'd like to label periods of low and high volatilty in a new column called ```VolLevel``` for each Symbol using some lower and upper bound values. For example, in the case of the Symbol D we'd like to have a new column with value determined by:\n",
    "\n",
    "```python\n",
    "if VolStrat > upper_threshold_dict['D']:\n",
    "    VolLevel = '3_HIGH'\n",
    "elif VolStrat < lower_threshold_dict['D']:\n",
    "    VolLevel = '1_LOW'\n",
    "else:\n",
    "    VolLevel = '2_MEDIUM'\n",
    "```\n",
    "\n",
    "Namely, this labelling should be applied to each row, and the threshold values much correspond to the Symbol for that row. We will group on this column and see if we can find any new insights in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine lower bounds (we choose to use 25th percentile)\n",
    "lower_threshold_dict = clean_df.groupby('Symbol')['VolStat'].quantile(0.25).to_dict() # 25th percentile bound\n",
    "lower_threshold_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine upper bounds (we choose to use 75th percentile)\n",
    "upper_threshold_dict = clean_df.groupby('Symbol')['VolStat'].quantile(0.75).to_dict() # 75th percentile bound\n",
    "upper_threshold_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our desire is to label low, medium, and high volatility periods so let's define a new column called ```VolLevel``` for each Symbol using some lower and upper bound values. Let's define a custom function that will be applied to each row to achieve this goal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom function, input is a row from the agg_df, and the output is a string, either LOW, MEDIUM, or HIGH\n",
    "def my_custom_row_function(row):\n",
    "    row_symbol = row['Symbol']    # the Symbol value in the row\n",
    "    row_volstat = row['VolStat']  # the VolStat value in the row\n",
    "    \n",
    "    lower_threshold = lower_threshold_dict[row_symbol] # Dictionary of {string:float}\n",
    "    upper_threshold = upper_threshold_dict[row_symbol] # Dictionary of {string:float}\n",
    "    \n",
    "    # The function decision, return value depending on low, medium, or high volatility\n",
    "    if row_volstat > upper_threshold:\n",
    "        return '3_HIGH'\n",
    "    elif row_volstat < lower_threshold:\n",
    "        return '1_LOW'\n",
    "    else:\n",
    "        return '2_MEDIUM'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now apply the function to each row of the DataFrame ```clean_df``` using a ```lambda``` statement. We store the returned values in the new column ```VolLevel```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply my_custom_row_function to the Pandas DataFrame, row by row (axis=1)\n",
    "clean_df['VolLevel'] = clean_df.apply(lambda row: my_custom_row_function(row), axis=1)\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the workflow here may seem complex at first, the ability to apply custom functions, group by certain features, and construct summary statistics will prove invaluable as you progress onto more advanced analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4:\n",
    "\n",
    "Using ```clean_df``` and a ```lambda``` statement within ```apply()```, write a function ```new_custom_function()``` and a script to add a new column to the DataFrame (call it ```EnhancedVolLevel```) that operates in a similar manner as VolLevel but instead gives five volatility level categories using the following logic to determine the label for the volatility level:\n",
    "\n",
    "```python\n",
    "if VolStrat > 90th percentile:\n",
    "    VolLevel = '5_VERY_HIGH'\n",
    "elif VolStrat > 75th percentile:\n",
    "    VolLevel = '4_HIGH'\n",
    "elif VolStrat > 25th percentile:\n",
    "    VolLevel = '3_MEDIUM'\n",
    "elif VolStrat > 10th percentile:\n",
    "    VolLevel = '2_LOW'\n",
    "else:\n",
    "    VolLevel = '1_VERY_LOW'\n",
    "```\n",
    "\n",
    "Remember each percentile should be calculated by symbol. Use these new labels to see if there are any patterns between volatility levels and the direction of returns. Hence, produce the DataFrame to be able to run the following command:\n",
    "\n",
    "```python\n",
    "clean_df.groupby(['Symbol','EnhancedVolLevel'])['ReturnFlag'].mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# One possible solution\n",
    "def new_custom_function(row):\n",
    "    row_symbol = row['Symbol']    # the Symbol value in the row\n",
    "    row_volstat = row['VolStat']  # the VolStat value in the row\n",
    "    \n",
    "    very_lower_threshold = very_lower_threshold_dict[row_symbol] # Dictionary of {string:float}\n",
    "    lower_threshold = lower_threshold_dict[row_symbol] # Dictionary of {string:float}\n",
    "    upper_threshold = upper_threshold_dict[row_symbol] # Dictionary of {string:float}\n",
    "    very_upper_threshold = very_upper_threshold_dict[row_symbol] # Dictionary of {string:float}\n",
    "    \n",
    "    # The function decision, return value depending on very low, low, medium, high, or very high volatility\n",
    "    if row_volstat > very_upper_threshold:\n",
    "        return '5_VERY_HIGH'\n",
    "    elif row_volstat > upper_threshold:\n",
    "        return '4_HIGH'\n",
    "    elif row_volstat > lower_threshold:\n",
    "        return '3_MEDIUM'\n",
    "    elif row_volstat > very_lower_threshold:\n",
    "        return '2_LOW'\n",
    "    else:\n",
    "        return '1_VERY_LOW'\n",
    "    \n",
    "# Script\n",
    "very_upper_threshold_dict = clean_df.groupby('Symbol')['VolStat'].quantile(0.90).to_dict() # 25th percentile bound\n",
    "upper_threshold_dict = clean_df.groupby('Symbol')['VolStat'].quantile(0.75).to_dict() # 25th percentile bound\n",
    "lower_threshold_dict = clean_df.groupby('Symbol')['VolStat'].quantile(0.25).to_dict() # 25th percentile bound\n",
    "very_lower_threshold_dict = clean_df.groupby('Symbol')['VolStat'].quantile(0.10).to_dict() # 25th percentile bound\n",
    "\n",
    "# Calculate new column\n",
    "clean_df['EnhancedVolLevel'] = clean_df.apply(lambda row: new_custom_function(row), axis=1)\n",
    "print(clean_df.head())\n",
    "\n",
    "# Run command\n",
    "clean_df.groupby(['Symbol','EnhancedVolLevel'])['ReturnFlag'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that volatility and stock returns do not exhibit any strong patterns in terms of the average return direction (positive or negative) for a given volatility regime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing stock returns to broader market returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look into the second part of your boss's question: what is the relationship between broader market returns and the returns of these five energy stocks? The S&P 500 Index is a stock index comprised of about 500 large-capitalization public US companies. The index is often used as a representation of the US stock market. If we can determine whether or not there exists a strong relationship between these 5 energy stocks' returns and those of the S&P 500 Index, we can determine if there are signficant idiosyncratic characteristics at play among the energy sector stock returns, or if the returns are solely driven by the broader market.\n",
    "\n",
    "Market returns for the tradable S&P 500 index ETF (exchange-traded fund) are available in ```SPY.csv``` (the \"stock\" symbol of the ETF is SPY). Let's load the data and append the daily returns onto the cleaned energy sector data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file into DataFrame\n",
    "market_df = pd.read_csv('SPY.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market_df['Symbol'] = 'SPY' # add column for symbol\n",
    "market_df['Return'] = (market_df['Close'] / market_df['Open']) - 1.0 # calculate return\n",
    "market_df['VolStat'] = (market_df['High'] - market_df['Low']) / market_df['Open']\n",
    "market_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to merge the market returns in ```market_df``` onto the energy stock data in ```clean_df```.  This can be accomplished using ```pd.merge()``` - a versatile method to join together DataFrames.\n",
    "\n",
    "For those of you familiar with SQL, merging and joining DataFrames can be accomplish in much the same ways that SQL accomplishes these tasks (if you are not familiar with SQL, don't worry - we will cover it in later cases). In this case, we'd like to use the intersection of dates in ```clean_df``` and ```market_df``` dates as the indices in the merge (i.e. in SQL parlance, we will perform an ```inner``` merge):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge inner (merge market_df onto clean_df using the dates of clean_df as the keys)\n",
    "merged_df = pd.merge(clean_df, market_df[['Date','Return']], how='inner', on='Date', suffixes=('','_SPY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many dates are in the intersection\n",
    "merged_df.groupby('Symbol')['Date'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5:\n",
    "\n",
    "Using only Symbol D in ```clean_df``` and ```market_df```, use ```pd.merge()``` to determine how many dates are in ```clean_df``` that are not in ```market_df```. Additionally, how many dates are in ```market_df``` that are not in ```clean_df```? (Hint: isnull() may be useful to simplify the solution.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One possible solution\n",
    "\n",
    "outer_df = pd.merge(clean_df[clean_df['Symbol'] == 'D'][['Date','Return']], market_df[['Date','Return']], how='outer', on='Date', suffixes=('','_SPY'))\n",
    "print('--- outer merge HEAD ---')\n",
    "print(outer_df.head())\n",
    "print('--- outer merge TAIL ---')\n",
    "print(outer_df.tail())\n",
    "print('--- outer merge NaN count ---')\n",
    "outer_df.isnull().sum()\n",
    "\n",
    "# Answer to: For Symbol D, how many dates are in clean_df that are not in market_df? -> 14\n",
    "# Answer to: For Symbol D, how many dates are in market_df that are not in clean_df? -> 81"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking down returns based on the broader market return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can begin our granular analysis of how broader market returns are related to single stock returns. \n",
    "\n",
    "Let's begin by breaking down broader market returns into quantiles. This quantile analysis approach we will take is commonly employed in data analysis to determine how the magnitude of one variable is related to another variable of interest.\n",
    "\n",
    "We can explore this idea with the ```pd.qcut()``` method. Namely, ```pd.qcut()``` will allow us to cut the market returns by quantiles (we will eventually group by quantiles), and therefore will allow us to calculate summary statistics (such as average return) for each quantile.\n",
    "\n",
    "Let's first extract the returns using the convenience of the ```pivot()``` method on a DataFrame. Pivoting a DataFrame may be accomplished by specifying:\n",
    "\n",
    "1. An index to pivot on. In this case we choose ```Date```.\n",
    "2. Columns that we'd like to have after the pivot. In this case we'd like columns that are the ````Symbol```.\n",
    "3. The values that each (row,column) pair will show. In this case we'd like to have the ```Return```.\n",
    "\n",
    "We will pivot ```merged_df``` using these parameter inputs to output a DataFrame where the rows are the Date, each column is the Symbol, and the values are the open-to-close daily return for the given date and symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract returns from merged_df, where we use pivot to simplify the task\n",
    "return_df = merged_df.pivot(index='Date', columns='Symbol', values='Return')\n",
    "return_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge on the broader market returns from the SPY ```market_df``` loaded earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's merge the SPY (broader market) returns by Date onto the return_df DataFrame\n",
    "full_df = pd.merge(return_df, market_df[['Date','Return']].set_index('Date'), left_index=True, right_index=True)\n",
    "full_df = full_df.rename(columns={'Return':'MarketReturn'}).reset_index()\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through a few simple lines we've created a DataFrame ```full_df``` where each value is an open-to-close daily return, whether it be for one of the five symbols under study, or for the broader market.\n",
    "\n",
    "We proceed by utilizing ```pd.qcut()``` with 10 quantiles. In general, the number of quantiles should be chosed based on how granular of a view one requires. Here, we will go with 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 10 quantile categories by the market return\n",
    "num_quantiles = 10\n",
    "full_df['market_quantile'] = pd.qcut(full_df['MarketReturn'],num_quantiles,labels=False)\n",
    "full_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's group by ```market_quantile``` and calculate the mean return for all the symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group by market quantile and calculate the mean return\n",
    "full_df.groupby('market_quantile').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each value in the above DataFrame output is a mean of daily returns for a given symbol, where the mean is taken across all dates that correspond to the ```market_quantile``` listed as the index of the output DataFrame. Note that higher quantile numbers indicate higher market returns, while lower quantile numbers indicate lower market returns (in this case, negative market returns).\n",
    "\n",
    "We see here that the energy stock returns do indeed follow a pattern when the SPY returns are large (higher quantile number) or small (lower quantile number). Namely, stock returns of the individual stocks follow the same pattern as those of the broader market. Hence, the broader market has an effect on the single-stock returns; that is, the magnitude of the broader stock market return is correlated with the magnitude of the return of the individual stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6:\n",
    "\n",
    "We created quantiles for market returns and subsequently calculated the mean return for each of these quantiles. Perform a similar analysis as above, but instead group by quantiles for the market volatility rather than market returns, and calculate the standard deviation of returns for each market volatility quantile category rather than the mean return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One possible solution\n",
    "# Merge market volstat onto returns\n",
    "new_df = pd.merge(return_df, market_df[['Date','VolStat']].set_index('Date'), left_index=True, right_index=True)\n",
    "new_df = new_df.rename(columns={'VolStat':'MarketVolStat'}).reset_index()\n",
    "\n",
    "# Add on quantiles\n",
    "num_quantiles = 10\n",
    "new_df['market_volstat_quantile'] = pd.qcut(new_df['MarketVolStat'],num_quantiles,labels=False)\n",
    "\n",
    "# Calcualte mean by quantile\n",
    "new_df.groupby('market_volstat_quantile').std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to how stock returns' directions following those of the broader market, the volatility of stock returns also follow the volatility of returns of the broader market. This warrants further analysis of the root cause of this effect as a future project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've explored stock returns for the five energy sector stocks in terms of their own volatility regimes, and their returns and volatility relative to the broader market. We found that for the stocks under study, there is no strong link between volatility level and the direction of the daily stock return. Moreover, we found that when comparing stocks to the broader market, their returns and volatility levels are amplified when the market returns and volatility levels are high. These findings indicate there is an intrinsic link between returns and volatility, both in the single-stock case and the broader market case. This lends a variety of avenues of exploration for follow-up projects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, you learned multiple data manipulation tools in ```pandas```, including anonymous functions, grouping, merging, quantile cuttting, and pivoting, while making use of data transformation and aggegation analysis techniques that we've previously learned.\n",
    "\n",
    "```pandas``` is an increbily versatile package and can significantly increase producivity and deliver exceptional business insights. These techniques should serve as a strong basis for any future data analyses you may conduct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
